{
	"jobConfig": {
		"name": "covid-transform-job",
		"description": "",
		"role": "arn:aws:iam::954976318953:role/AWSGlueETLRole",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "covid-transform-job.py",
		"scriptLocation": "s3://aws-glue-assets-954976318953-eu-west-3/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-06-13T12:03:42.418Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-954976318953-eu-west-3/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-954976318953-eu-west-3/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null,
		"pythonPath": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nfrom awsglue.transforms import *\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nfrom pyspark.sql.functions import col, to_date\r\nfrom pyspark.sql.types import DoubleType\r\nfrom awsglue.dynamicframe import DynamicFrame\r\n\r\n# Initialize Glue job\r\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\njob.init(args['JOB_NAME'], args)\r\n\r\n# Step 1: Read data from Glue catalog\r\ninput_dyf = glueContext.create_dynamic_frame.from_catalog(\r\n    database=\"covid-db\",\r\n    table_name=\"covid_data_raw\",\r\n    transformation_ctx=\"input_dyf\"\r\n)\r\n\r\n# Step 2: Convert to DataFrame\r\ndf = input_dyf.toDF()\r\n\r\n# Step 3: Select relevant columns\r\nselected_cols = [\r\n    \"iso_code\", \"continent\", \"location\", \"date\", \"total_cases\",\r\n    \"new_cases\", \"total_deaths\", \"new_deaths\", \"people_vaccinated\",\r\n    \"people_fully_vaccinated\", \"population\"\r\n]\r\ndf = df.select(*selected_cols)\r\n\r\n# Step 4: Convert data types\r\ndf = df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\r\nfor col_name in [\r\n    \"total_cases\", \"new_cases\", \"total_deaths\", \"new_deaths\",\r\n    \"people_vaccinated\", \"people_fully_vaccinated\", \"population\"\r\n]:\r\n    df = df.withColumn(col_name, col(col_name).cast(DoubleType()))\r\n\r\n# Step 5: Drop rows with nulls in essential fields and filter iso-code for country\r\ndf = df.dropna(subset=[\"location\", \"date\", \"total_cases\",\"continent\"])\r\ndf = df.filter(\r\n    (df['iso_code'].isNotNull()) & (~df['iso_code'].startswith('OWID_')))\r\n    \r\n# Step 6: Convert back to DynamicFrame\r\nfinal_dyf = DynamicFrame.fromDF(df, glueContext, \"final_dyf\")\r\n\r\n# Step 7: Write the cleaned data to S3 in Parquet format\r\nglueContext.write_dynamic_frame.from_options(\r\n    frame=final_dyf,\r\n    connection_type=\"s3\",\r\n    connection_options={\"path\": \"s3://my-covid-etl-bucket/cleaned/\"},\r\n    format=\"parquet\"\r\n)\r\n\r\njob.commit()"
}